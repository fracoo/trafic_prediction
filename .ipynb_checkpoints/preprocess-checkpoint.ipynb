{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8nd0m82sl6x",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è pmdarima non disponible: ValueError\n",
      "   ‚Üí Si besoin, r√©installer: pip uninstall pmdarima -y && pip install pmdarima\n",
      "================================================================================\n",
      "‚úÖ BIBLIOTH√àQUES CHARG√âES\n",
      "================================================================================\n",
      "üìä Pandas version: 2.2.3\n",
      "üî¢ NumPy version: 2.1.3\n",
      "ü§ñ Scikit-learn version: 1.6.1\n",
      "üöÄ XGBoost version: 3.1.1\n",
      "üí° LightGBM version: 4.6.0\n",
      "üß† TensorFlow version: 2.19.0\n",
      "üî• PyTorch version: 2.6.0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS - TOUTES LES BIBLIOTH√àQUES N√âCESSAIRES\n",
    "# ============================================================================\n",
    "\n",
    "# Manipulation de donn√©es\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Preprocessing\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Mod√®les de Machine Learning - Classiques\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# XGBoost et LightGBM\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    from lightgbm import LGBMRegressor\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LightGBM non install√©. Pour l'installer: pip install lightgbm\")\n",
    "\n",
    "# CatBoost\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è CatBoost non install√©. Pour l'installer: pip install catboost\")\n",
    "\n",
    "# R√©seaux de neurones - TensorFlow/Keras\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import Sequential, Model\n",
    "    from tensorflow.keras.layers import Dense, Dropout, LSTM, GRU, Conv1D, MaxPooling1D, Flatten\n",
    "    from tensorflow.keras.layers import BatchNormalization, Activation, Input, Concatenate\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "    from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è TensorFlow non install√©. Pour l'installer: pip install tensorflow\")\n",
    "\n",
    "# R√©seaux de neurones - PyTorch (alternative)\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PyTorch non install√©. Pour l'installer: pip install torch\")\n",
    "\n",
    "# Mod√®les de s√©ries temporelles\n",
    "try:\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Statsmodels non install√©. Pour l'installer: pip install statsmodels\")\n",
    "\n",
    "# pmdarima (peut avoir des probl√®mes de compatibilit√© avec certaines versions de numpy)\n",
    "try:\n",
    "    from pmdarima import auto_arima\n",
    "except (ImportError, ValueError) as e:\n",
    "    print(f\"‚ö†Ô∏è pmdarima non disponible: {type(e).__name__}\")\n",
    "    print(\"   ‚Üí Si besoin, r√©installer: pip uninstall pmdarima -y && pip install pmdarima\")\n",
    "\n",
    "# Prophet (Facebook)\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Prophet non install√©. Pour l'installer: pip install prophet\")\n",
    "\n",
    "# Suppression des warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration de l'affichage\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ BIBLIOTH√àQUES CHARG√âES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
    "print(f\"ü§ñ Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"üöÄ XGBoost version: {xgb.__version__}\")\n",
    "try:\n",
    "    print(f\"üí° LightGBM version: {lgb.__version__}\")\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    print(f\"üß† TensorFlow version: {tf.__version__}\")\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "except:\n",
    "    pass\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ty0qh5uonx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Chargement des fichiers...\n",
      "‚úÖ Donn√©es de trafic charg√©es: (9266, 15)\n",
      "\n",
      "üìã Aper√ßu des donn√©es:\n",
      "   Identifiant arc            Libelle  Date et heure de comptage  \\\n",
      "0             4264  AV_Champs_Elysees  2024-12-09T05:00:00+01:00   \n",
      "1             4264  AV_Champs_Elysees  2024-12-09T06:00:00+01:00   \n",
      "2             4264  AV_Champs_Elysees  2024-12-09T09:00:00+01:00   \n",
      "3             4264  AV_Champs_Elysees  2025-09-02T09:00:00+02:00   \n",
      "4             4264  AV_Champs_Elysees  2024-09-04T20:00:00+02:00   \n",
      "\n",
      "   D√©bit horaire  Taux d'occupation Etat trafic  Identifiant noeud amont  \\\n",
      "0          199.0            2.20945      Fluide                     2294   \n",
      "1          235.0            2.28778      Fluide                     2294   \n",
      "2         1041.0           11.63222      Fluide                     2294   \n",
      "3         1139.0           28.39222  Pr√©-satur√©                     2294   \n",
      "4          686.0           13.21611      Fluide                     2294   \n",
      "\n",
      "            Libelle noeud amont  Identifiant noeud aval  \\\n",
      "0  Av_Champs_Elysees-Washington                    2293   \n",
      "1  Av_Champs_Elysees-Washington                    2293   \n",
      "2  Av_Champs_Elysees-Washington                    2293   \n",
      "3  Av_Champs_Elysees-Washington                    2293   \n",
      "4  Av_Champs_Elysees-Washington                    2293   \n",
      "\n",
      "        Libelle noeud aval  Etat arc Date debut dispo data  \\\n",
      "0  Av_Champs_Elysees-Berri  Invalide            1996-10-10   \n",
      "1  Av_Champs_Elysees-Berri  Invalide            1996-10-10   \n",
      "2  Av_Champs_Elysees-Berri  Invalide            1996-10-10   \n",
      "3  Av_Champs_Elysees-Berri    Ouvert            1996-10-10   \n",
      "4  Av_Champs_Elysees-Berri  Invalide            1996-10-10   \n",
      "\n",
      "  Date fin dispo data                           geo_point_2d  \\\n",
      "0          2023-01-01  48.87153587897718, 2.3017227924560624   \n",
      "1          2023-01-01  48.87153587897718, 2.3017227924560624   \n",
      "2          2023-01-01  48.87153587897718, 2.3017227924560624   \n",
      "3          2023-01-01  48.87153587897718, 2.3017227924560624   \n",
      "4          2023-01-01  48.87153587897718, 2.3017227924560624   \n",
      "\n",
      "                                           geo_shape  \n",
      "0  {\"coordinates\": [[2.3009951475338775, 48.87177...  \n",
      "1  {\"coordinates\": [[2.3009951475338775, 48.87177...  \n",
      "2  {\"coordinates\": [[2.3009951475338775, 48.87177...  \n",
      "3  {\"coordinates\": [[2.3009951475338775, 48.87177...  \n",
      "4  {\"coordinates\": [[2.3009951475338775, 48.87177...  \n",
      "\n",
      "üìä Info sur les colonnes:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9266 entries, 0 to 9265\n",
      "Data columns (total 15 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Identifiant arc            9266 non-null   int64  \n",
      " 1   Libelle                    9266 non-null   object \n",
      " 2   Date et heure de comptage  9266 non-null   object \n",
      " 3   D√©bit horaire              8703 non-null   float64\n",
      " 4   Taux d'occupation          8644 non-null   float64\n",
      " 5   Etat trafic                9266 non-null   object \n",
      " 6   Identifiant noeud amont    9266 non-null   int64  \n",
      " 7   Libelle noeud amont        9266 non-null   object \n",
      " 8   Identifiant noeud aval     9266 non-null   int64  \n",
      " 9   Libelle noeud aval         9266 non-null   object \n",
      " 10  Etat arc                   9266 non-null   object \n",
      " 11  Date debut dispo data      9266 non-null   object \n",
      " 12  Date fin dispo data        9266 non-null   object \n",
      " 13  geo_point_2d               9266 non-null   object \n",
      " 14  geo_shape                  9266 non-null   object \n",
      "dtypes: float64(2), int64(3), object(10)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHARGEMENT DES DONN√âES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìÇ Chargement des fichiers...\")\n",
    "\n",
    "# Charger les donn√©es de trafic principales\n",
    "df_model = pd.read_csv('dataset_brut/champs_elysees.csv', sep=';')\n",
    "print(f\"‚úÖ Donn√©es de trafic charg√©es: {df_model.shape}\")\n",
    "\n",
    "# Afficher les premi√®res lignes\n",
    "print(\"\\nüìã Aper√ßu des donn√©es:\")\n",
    "print(df_model.head())\n",
    "\n",
    "print(\"\\nüìä Info sur les colonnes:\")\n",
    "print(df_model.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7934byg35l2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Renommage des colonnes...\n",
      "\n",
      "üìã Colonnes AVANT renommage:\n",
      "  1. Identifiant arc\n",
      "  2. Libelle\n",
      "  3. Date et heure de comptage\n",
      "  4. D√©bit horaire\n",
      "  5. Taux d'occupation\n",
      "  6. Etat trafic\n",
      "  7. Identifiant noeud amont\n",
      "  8. Libelle noeud amont\n",
      "  9. Identifiant noeud aval\n",
      "  10. Libelle noeud aval\n",
      "  ... (15 colonnes au total)\n",
      "\n",
      "‚úÖ Colonnes APR√àS renommage:\n",
      "  1. identifiant_arc\n",
      "  2. libelle\n",
      "  3. date_et_heure_de_comptage\n",
      "  4. debit_horaire\n",
      "  5. taux_doccupation\n",
      "  6. etat_trafic\n",
      "  7. identifiant_noeud_amont\n",
      "  8. libelle_noeud_amont\n",
      "  9. identifiant_noeud_aval\n",
      "  10. libelle_noeud_aval\n",
      "  11. etat_arc\n",
      "  12. date_debut_dispo_data\n",
      "  13. date_fin_dispo_data\n",
      "  14. geo_point_2d\n",
      "  15. geo_shape\n",
      "\n",
      "üìä Shape: (9266, 15)\n",
      "üìä Types de donn√©es:\n",
      "identifiant_arc                int64\n",
      "libelle                       object\n",
      "date_et_heure_de_comptage     object\n",
      "debit_horaire                float64\n",
      "taux_doccupation             float64\n",
      "etat_trafic                   object\n",
      "identifiant_noeud_amont        int64\n",
      "libelle_noeud_amont           object\n",
      "identifiant_noeud_aval         int64\n",
      "libelle_noeud_aval            object\n",
      "etat_arc                      object\n",
      "date_debut_dispo_data         object\n",
      "date_fin_dispo_data           object\n",
      "geo_point_2d                  object\n",
      "geo_shape                     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RENOMMAGE DES COLONNES - Nettoyage et standardisation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîß Renommage des colonnes...\")\n",
    "\n",
    "# Fonction de nettoyage des noms de colonnes\n",
    "def nettoyer_nom_colonne(nom):\n",
    "    \"\"\"Nettoie le nom de colonne: supprime espaces, accents, apostrophes\"\"\"\n",
    "    import unicodedata\n",
    "    # Supprimer les accents\n",
    "    nom = unicodedata.normalize('NFD', nom)\n",
    "    nom = nom.encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Remplacer espaces par underscore, supprimer apostrophes\n",
    "    nom = nom.replace(' ', '_').replace(\"'\", '').replace('-', '_')\n",
    "    # Mettre en minuscules\n",
    "    nom = nom.lower()\n",
    "    return nom\n",
    "\n",
    "# Afficher les colonnes AVANT renommage\n",
    "print(\"\\nüìã Colonnes AVANT renommage:\")\n",
    "for i, col in enumerate(df_model.columns[:10], 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "if len(df_model.columns) > 10:\n",
    "    print(f\"  ... ({len(df_model.columns)} colonnes au total)\")\n",
    "\n",
    "# Renommer toutes les colonnes\n",
    "df_model.columns = [nettoyer_nom_colonne(col) for col in df_model.columns]\n",
    "\n",
    "print(\"\\n‚úÖ Colonnes APR√àS renommage:\")\n",
    "for i, col in enumerate(df_model.columns[:15], 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "if len(df_model.columns) > 15:\n",
    "    print(f\"  ... ({len(df_model.columns)} colonnes au total)\")\n",
    "\n",
    "# Afficher les types de donn√©es\n",
    "print(f\"\\nüìä Shape: {df_model.shape}\")\n",
    "print(f\"üìä Types de donn√©es:\")\n",
    "print(df_model.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dmiaqn5emp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  Suppression des colonnes inutiles...\n",
      "  ‚ùå etat_trafic\n",
      "  ‚ùå identifiant_noeud_amont\n",
      "  ‚ùå libelle_noeud_amont\n",
      "  ‚ùå identifiant_noeud_aval\n",
      "  ‚ùå libelle_noeud_aval\n",
      "  ‚ùå etat_arc\n",
      "  ‚ùå date_debut_dispo_data\n",
      "  ‚ùå date_fin_dispo_data\n",
      "  ‚ùå geo_point_2d\n",
      "  ‚ùå geo_shape\n",
      "\n",
      "‚úÖ 10 colonnes supprim√©es\n",
      "üìä Shape apr√®s nettoyage: (9266, 5)\n",
      "\n",
      "================================================================================\n",
      "VALEURS MANQUANTES PAR COLONNE\n",
      "================================================================================\n",
      "         colonne  nb_na   pct_na\n",
      "taux_doccupation    622 6.712713\n",
      "   debit_horaire    563 6.075977\n",
      "\n",
      "üìã Colonnes conserv√©es (5):\n",
      "  ‚úì identifiant_arc\n",
      "  ‚úì libelle\n",
      "  ‚úì date_et_heure_de_comptage\n",
      "  ‚úì debit_horaire\n",
      "  ‚úì taux_doccupation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SUPPRESSION DES COLONNES INUTILES & ANALYSE DES VALEURS MANQUANTES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üóëÔ∏è  Suppression des colonnes inutiles...\")\n",
    "\n",
    "# Colonnes √† supprimer (non pertinentes pour l'entra√Ænement)\n",
    "colonnes_a_supprimer = [\n",
    "    'etat_trafic',              # Data leakage (corr√©l√© avec la cible)\n",
    "    'identifiant_noeud_amont',  # Identifiant technique\n",
    "    'libelle_noeud_amont',      # Label technique\n",
    "    'identifiant_noeud_aval',   # Identifiant technique\n",
    "    'libelle_noeud_aval',       # Label technique\n",
    "    'etat_arc',                 # M√©tadonn√©e technique\n",
    "    'date_debut_dispo_data',    # M√©tadonn√©e\n",
    "    'date_fin_dispo_data',      # M√©tadonn√©e\n",
    "    'geo_point_2d',             # G√©olocalisation\n",
    "    'geo_shape',                # G√©om√©trie\n",
    "]\n",
    "\n",
    "# Supprimer les colonnes\n",
    "colonnes_supprimees = []\n",
    "for col in colonnes_a_supprimer:\n",
    "    if col in df_model.columns:\n",
    "        df_model = df_model.drop(columns=[col])\n",
    "        colonnes_supprimees.append(col)\n",
    "        print(f\"  ‚ùå {col}\")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(colonnes_supprimees)} colonnes supprim√©es\")\n",
    "print(f\"üìä Shape apr√®s nettoyage: {df_model.shape}\")\n",
    "\n",
    "# Analyse des valeurs manquantes\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VALEURS MANQUANTES PAR COLONNE\")\n",
    "print(\"=\" * 80)\n",
    "na_summary = pd.DataFrame({\n",
    "    'colonne': df_model.columns,\n",
    "    'nb_na': df_model.isna().sum().values,\n",
    "    'pct_na': (df_model.isna().sum() / len(df_model) * 100).values\n",
    "})\n",
    "na_summary = na_summary[na_summary['nb_na'] > 0].sort_values('pct_na', ascending=False)\n",
    "\n",
    "if len(na_summary) > 0:\n",
    "    print(na_summary.to_string(index=False))\n",
    "else:\n",
    "    print(\"‚úÖ Aucune valeur manquante!\")\n",
    "\n",
    "print(f\"\\nüìã Colonnes conserv√©es ({len(df_model.columns)}):\")\n",
    "for col in df_model.columns:\n",
    "    print(f\"  ‚úì {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ii9oyla4j99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Conversion de la colonne date_et_heure_de_comptage...\n",
      "  ‚úì Colonne renomm√©e: 'date_et_heure_de_comptage' ‚Üí 'date_heure_comptage'\n",
      "\n",
      "üîß Extraction des features temporelles...\n",
      "‚úÖ Features temporelles cr√©√©es:\n",
      "  ‚úì heure\n",
      "  ‚úì jour_semaine\n",
      "  ‚úì jour_mois\n",
      "  ‚úì mois\n",
      "  ‚úì annee\n",
      "  ‚úì semaine_annee\n",
      "  ‚úì est_weekend\n",
      "  ‚úì heure_sin\n",
      "  ‚úì heure_cos\n",
      "  ‚úì jour_semaine_sin\n",
      "  ‚úì jour_semaine_cos\n",
      "  ‚úì mois_sin\n",
      "  ‚úì mois_cos\n",
      "\n",
      "üìä Shape: (9266, 18)\n",
      "üìÖ P√©riode des donn√©es: 2024-09-01 03:00:00 ‚Üí 2025-10-29 23:00:00\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONVERSION DATETIME & EXTRACTION DES FEATURES TEMPORELLES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìÖ Conversion de la colonne date_et_heure_de_comptage...\")\n",
    "\n",
    "# Conversion en datetime (gestion du timezone)\n",
    "df_model['date_et_heure_de_comptage'] = pd.to_datetime(df_model['date_et_heure_de_comptage'], utc=True, errors='coerce')\n",
    "\n",
    "# Supprimer le timezone pour faciliter les manipulations\n",
    "if df_model['date_et_heure_de_comptage'].dt.tz is not None:\n",
    "    df_model['date_et_heure_de_comptage'] = df_model['date_et_heure_de_comptage'].dt.tz_localize(None)\n",
    "\n",
    "# Supprimer les lignes avec des dates invalides\n",
    "nb_avant = len(df_model)\n",
    "df_model = df_model.dropna(subset=['date_et_heure_de_comptage'])\n",
    "nb_apres = len(df_model)\n",
    "if nb_avant > nb_apres:\n",
    "    print(f\"  ‚ÑπÔ∏è  {nb_avant - nb_apres} lignes supprim√©es (dates invalides)\")\n",
    "\n",
    "# Renommer en 'date_heure_comptage' pour simplifier\n",
    "df_model = df_model.rename(columns={'date_et_heure_de_comptage': 'date_heure_comptage'})\n",
    "print(f\"  ‚úì Colonne renomm√©e: 'date_et_heure_de_comptage' ‚Üí 'date_heure_comptage'\")\n",
    "\n",
    "# Extraction des features temporelles\n",
    "print(\"\\nüîß Extraction des features temporelles...\")\n",
    "df_model['heure'] = df_model['date_heure_comptage'].dt.hour\n",
    "df_model['jour_semaine'] = df_model['date_heure_comptage'].dt.dayofweek  # 0=Lundi, 6=Dimanche\n",
    "df_model['jour_mois'] = df_model['date_heure_comptage'].dt.day\n",
    "df_model['mois'] = df_model['date_heure_comptage'].dt.month\n",
    "df_model['annee'] = df_model['date_heure_comptage'].dt.year\n",
    "df_model['semaine_annee'] = df_model['date_heure_comptage'].dt.isocalendar().week\n",
    "df_model['est_weekend'] = (df_model['jour_semaine'] >= 5).astype(int)  # Samedi=5, Dimanche=6\n",
    "\n",
    "# Features cycliques (pour capturer la nature cyclique du temps)\n",
    "df_model['heure_sin'] = np.sin(2 * np.pi * df_model['heure'] / 24)\n",
    "df_model['heure_cos'] = np.cos(2 * np.pi * df_model['heure'] / 24)\n",
    "df_model['jour_semaine_sin'] = np.sin(2 * np.pi * df_model['jour_semaine'] / 7)\n",
    "df_model['jour_semaine_cos'] = np.cos(2 * np.pi * df_model['jour_semaine'] / 7)\n",
    "df_model['mois_sin'] = np.sin(2 * np.pi * df_model['mois'] / 12)\n",
    "df_model['mois_cos'] = np.cos(2 * np.pi * df_model['mois'] / 12)\n",
    "\n",
    "print(\"‚úÖ Features temporelles cr√©√©es:\")\n",
    "features_temps = ['heure', 'jour_semaine', 'jour_mois', 'mois', 'annee', 'semaine_annee', \n",
    "                  'est_weekend', 'heure_sin', 'heure_cos', 'jour_semaine_sin', \n",
    "                  'jour_semaine_cos', 'mois_sin', 'mois_cos']\n",
    "for f in features_temps:\n",
    "    print(f\"  ‚úì {f}\")\n",
    "\n",
    "print(f\"\\nüìä Shape: {df_model.shape}\")\n",
    "print(f\"üìÖ P√©riode des donn√©es: {df_model['date_heure_comptage'].min()} ‚Üí {df_model['date_heure_comptage'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "496edd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3\n",
      "['/Library/Frameworks/Python.framework/Versions/3.11/lib/python311.zip', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/lib-dynload', '', '/Users/macbookair/Library/Python/3.11/lib/python/site-packages', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sooc7nbs4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INT√âGRATION DES DONN√âES EXTERNES\n",
      "================================================================================\n",
      "\n",
      "üìö Chargement des vacances scolaires...\n",
      "‚úÖ 2306 p√©riodes de vacances charg√©es\n",
      "\n",
      "üìã Aper√ßu des vacances:\n",
      "                Description Date de d√©but Date de fin\n",
      "0  Vacances de la Toussaint           NaT         NaT\n",
      "1  Vacances de la Toussaint           NaT         NaT\n",
      "2          Vacances de No√´l           NaT         NaT\n",
      "3          Vacances d'Hiver           NaT         NaT\n",
      "4     Vacances de Printemps           NaT         NaT\n",
      "\n",
      "üîÑ Application de la d√©tection des vacances...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Appliquer la fonction\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîÑ Application de la d√©tection des vacances...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m df_model[\u001b[33m'\u001b[39m\u001b[33mest_vacances\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf_model\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate_heure_comptage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest_en_vacances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Vacances d√©tect√©es: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_model[\u001b[33m'\u001b[39m\u001b[33mest_vacances\u001b[39m\u001b[33m'\u001b[39m].sum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m observations (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_model[\u001b[33m'\u001b[39m\u001b[33mest_vacances\u001b[39m\u001b[33m'\u001b[39m].mean()*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# 2. JOURS F√âRI√âS\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/base.py:919\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    916\u001b[39m arr = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/arrays/_mixins.py:81\u001b[39m, in \u001b[36mravel_compat.<locals>.method\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(meth)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmethod\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     flags = \u001b[38;5;28mself\u001b[39m._ndarray.flags\n\u001b[32m     84\u001b[39m     flat = \u001b[38;5;28mself\u001b[39m.ravel(\u001b[33m\"\u001b[39m\u001b[33mK\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/arrays/datetimelike.py:740\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin.map\u001b[39m\u001b[34m(self, mapper, na_action)\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;129m@ravel_compat\u001b[39m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, mapper, na_action=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    738\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Index\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     result = \u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    741\u001b[39m     result = Index(result)\n\u001b[32m    743\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, ABCMultiIndex):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mest_en_vacances\u001b[39m\u001b[34m(date)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pd.isna(date):\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvacances_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate de d√©but\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate de fin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/frame.py:1554\u001b[39m, in \u001b[36mDataFrame.iterrows\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1552\u001b[39m using_cow = using_copy_on_write()\n\u001b[32m   1553\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.index, \u001b[38;5;28mself\u001b[39m.values):\n\u001b[32m-> \u001b[39m\u001b[32m1554\u001b[39m     s = \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mgr.is_single_block:\n\u001b[32m   1556\u001b[39m         s._mgr.add_references(\u001b[38;5;28mself\u001b[39m._mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/series.py:574\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, fastpath)\u001b[39m\n\u001b[32m    572\u001b[39m         data = [data]\n\u001b[32m    573\u001b[39m     index = default_index(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_list_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    575\u001b[39m     com.require_length_match(data, index)\n\u001b[32m    577\u001b[39m \u001b[38;5;66;03m# create/copy the manager\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# JOINTURE AVEC DONN√âES EXTERNES (Vacances, Jours F√©ri√©s, M√©t√©o)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INT√âGRATION DES DONN√âES EXTERNES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -------------------------\n",
    "# 1. VACANCES SCOLAIRES\n",
    "# -------------------------\n",
    "print(\"\\nüìö Chargement des vacances scolaires...\")\n",
    "vacances_df = pd.read_csv('dataset_brut/fr-en-calendrier-scolaire.csv', sep=';')\n",
    "\n",
    "# Convertir les dates\n",
    "vacances_df['Date de d√©but'] = pd.to_datetime(vacances_df['Date de d√©but'], format='%Y-%m-%d', errors='coerce')\n",
    "vacances_df['Date de fin'] = pd.to_datetime(vacances_df['Date de fin'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "print(f\"‚úÖ {len(vacances_df)} p√©riodes de vacances charg√©es\")\n",
    "print(\"\\nüìã Aper√ßu des vacances:\")\n",
    "print(vacances_df[['Description', 'Date de d√©but', 'Date de fin']].head())\n",
    "\n",
    "# Fonction pour v√©rifier si une date est en vacances\n",
    "def est_en_vacances(date):\n",
    "    if pd.isna(date):\n",
    "        return 0\n",
    "    for _, row in vacances_df.iterrows():\n",
    "        if row['Date de d√©but'] <= date <= row['Date de fin']:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Appliquer la fonction\n",
    "print(\"\\nüîÑ Application de la d√©tection des vacances...\")\n",
    "df_model['est_vacances'] = df_model['date_heure_comptage'].apply(est_en_vacances)\n",
    "print(f\"‚úÖ Vacances d√©tect√©es: {df_model['est_vacances'].sum()} observations ({df_model['est_vacances'].mean()*100:.1f}%)\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. JOURS F√âRI√âS\n",
    "# -------------------------\n",
    "print(\"\\nüéâ Cr√©ation de la liste des jours f√©ri√©s 2024-2025...\")\n",
    "\n",
    "jours_feries = [\n",
    "    '2024-01-01',  # Jour de l'an\n",
    "    '2024-04-01',  # Lundi de P√¢ques\n",
    "    '2024-05-01',  # F√™te du travail\n",
    "    '2024-05-08',  # Victoire 1945\n",
    "    '2024-05-09',  # Ascension\n",
    "    '2024-05-20',  # Lundi de Pentec√¥te\n",
    "    '2024-07-14',  # F√™te nationale\n",
    "    '2024-08-15',  # Assomption\n",
    "    '2024-11-01',  # Toussaint\n",
    "    '2024-11-11',  # Armistice 1918\n",
    "    '2024-12-25',  # No√´l\n",
    "    '2025-01-01',  # Jour de l'an\n",
    "    '2025-04-21',  # Lundi de P√¢ques\n",
    "    '2025-05-01',  # F√™te du travail\n",
    "    '2025-05-08',  # Victoire 1945\n",
    "    '2025-05-29',  # Ascension\n",
    "    '2025-06-09',  # Lundi de Pentec√¥te\n",
    "    '2025-07-14',  # F√™te nationale\n",
    "    '2025-08-15',  # Assomption\n",
    "    '2025-11-01',  # Toussaint\n",
    "    '2025-11-11',  # Armistice 1918\n",
    "    '2025-12-25',  # No√´l\n",
    "]\n",
    "\n",
    "jours_feries = pd.to_datetime(jours_feries)\n",
    "print(f\"‚úÖ {len(jours_feries)} jours f√©ri√©s d√©finis\")\n",
    "\n",
    "# V√©rifier si la date est un jour f√©ri√©\n",
    "df_model['est_jour_ferie'] = df_model['date_heure_comptage'].dt.date.astype('datetime64[ns]').isin(jours_feries).astype(int)\n",
    "print(f\"‚úÖ Jours f√©ri√©s d√©tect√©s: {df_model['est_jour_ferie'].sum()} observations ({df_model['est_jour_ferie'].mean()*100:.1f}%)\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. M√âT√âO\n",
    "# -------------------------\n",
    "print(\"\\nüå§Ô∏è  Chargement des donn√©es m√©t√©o...\")\n",
    "\n",
    "try:\n",
    "    meteo_df = pd.read_csv('dataset_brut/H_75_latest-2024-2025.csv', sep=';')\n",
    "    \n",
    "    # Convertir la date\n",
    "    meteo_df['AAAAMMJJHH'] = pd.to_datetime(meteo_df['AAAAMMJJHH'], format='%Y%m%d%H', errors='coerce')\n",
    "    \n",
    "    # S√©lectionner et renommer les colonnes pertinentes\n",
    "    # Note: le fichier a \"Prcipitation\" au lieu de \"Pr√©cipitation\" (erreur d'encodage)\n",
    "    colonnes_meteo = {\n",
    "        'AAAAMMJJHH': 'date_heure',\n",
    "        'T': 'temperature',\n",
    "        'U': 'humidite',\n",
    "        'Prcipitation en 1h': 'precipitation'\n",
    "    }\n",
    "    \n",
    "    meteo_df = meteo_df[list(colonnes_meteo.keys())].rename(columns=colonnes_meteo)\n",
    "    \n",
    "    # Convertir en num√©rique\n",
    "    meteo_df['temperature'] = pd.to_numeric(meteo_df['temperature'], errors='coerce')\n",
    "    meteo_df['humidite'] = pd.to_numeric(meteo_df['humidite'], errors='coerce')\n",
    "    meteo_df['precipitation'] = pd.to_numeric(meteo_df['precipitation'], errors='coerce')\n",
    "    \n",
    "    print(f\"‚úÖ Donn√©es m√©t√©o charg√©es: {meteo_df.shape}\")\n",
    "    print(f\"üìÖ P√©riode m√©t√©o: {meteo_df['date_heure'].min()} ‚Üí {meteo_df['date_heure'].max()}\")\n",
    "    \n",
    "    # Supprimer les doublons de date\n",
    "    meteo_df = meteo_df.drop_duplicates(subset=['date_heure'], keep='first')\n",
    "    \n",
    "    # Jointure avec les donn√©es de trafic\n",
    "    print(\"\\nüîó Jointure avec les donn√©es de trafic...\")\n",
    "    df_model = df_model.merge(\n",
    "        meteo_df,\n",
    "        left_on='date_heure_comptage',\n",
    "        right_on='date_heure',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Supprimer la colonne date_heure en double\n",
    "    df_model = df_model.drop(columns=['date_heure'], errors='ignore')\n",
    "    \n",
    "    # Afficher les statistiques de couverture\n",
    "    for col in ['temperature', 'humidite', 'precipitation']:\n",
    "        pct_valeurs = df_model[col].notna().sum() / len(df_model) * 100\n",
    "        print(f\"  ‚úì {col}: {pct_valeurs:.1f}% de couverture\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Jointure m√©t√©o r√©ussie!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Erreur lors du chargement de la m√©t√©o: {e}\")\n",
    "    print(\"  ‚Üí Les features m√©t√©o ne seront pas disponibles\")\n",
    "\n",
    "# -------------------------\n",
    "# R√âSUM√â FINAL\n",
    "# -------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"R√âSUM√â DES DONN√âES ENRICHIES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìä Shape finale: {df_model.shape}\")\n",
    "print(f\"üìÖ P√©riode: {df_model['date_heure_comptage'].min()} ‚Üí {df_model['date_heure_comptage'].max()}\")\n",
    "print(f\"\\nüìã Nouvelles features cr√©√©es:\")\n",
    "print(f\"  ‚úì est_vacances\")\n",
    "print(f\"  ‚úì est_jour_ferie\")\n",
    "if 'temperature' in df_model.columns:\n",
    "    print(f\"  ‚úì temperature\")\n",
    "    print(f\"  ‚úì humidite\")\n",
    "    print(f\"  ‚úì precipitation\")\n",
    "\n",
    "print(f\"\\nüìä Aper√ßu des donn√©es finales:\")\n",
    "print(df_model.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pvm18fpu27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALISATION - D√âBIT HORAIRE ET TAUX D'OCCUPATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìä Cr√©ation des visualisations...\")\n",
    "\n",
    "# Cr√©er une copie pour la visualisation (sans valeurs manquantes)\n",
    "df_viz = df_model.dropna(subset=['debit_horaire', 'taux_occupation']).copy()\n",
    "\n",
    "# Trier par date\n",
    "df_viz = df_viz.sort_values('date_heure_comptage')\n",
    "\n",
    "print(f\"‚úÖ Donn√©es pour visualisation: {len(df_viz)} observations\")\n",
    "print(f\"üìÖ P√©riode: {df_viz['date_heure_comptage'].min()} ‚Üí {df_viz['date_heure_comptage'].max()}\")\n",
    "\n",
    "# -------------------------\n",
    "# GRAPHIQUE 1: √âvolution temporelle\n",
    "# -------------------------\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Subplot 1: D√©bit horaire\n",
    "axes[0].plot(df_viz['date_heure_comptage'], df_viz['debit_horaire'], \n",
    "            color='steelblue', linewidth=0.8, alpha=0.7)\n",
    "axes[0].set_title('√âvolution du D√©bit Horaire (v√©hicules/heure)', \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "axes[0].set_xlabel('Date', fontsize=11)\n",
    "axes[0].set_ylabel('D√©bit horaire (v√©h/h)', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, linestyle='--')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Statistiques sur le graphique\n",
    "mean_debit = df_viz['debit_horaire'].mean()\n",
    "axes[0].axhline(y=mean_debit, color='red', linestyle='--', linewidth=1.5, \n",
    "               label=f'Moyenne: {mean_debit:.0f} v√©h/h', alpha=0.7)\n",
    "axes[0].legend(loc='upper right')\n",
    "\n",
    "# Subplot 2: Taux d'occupation\n",
    "axes[1].plot(df_viz['date_heure_comptage'], df_viz['taux_occupation'], \n",
    "            color='darkorange', linewidth=0.8, alpha=0.7)\n",
    "axes[1].set_title('√âvolution du Taux d\\'Occupation (%)', \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "axes[1].set_xlabel('Date', fontsize=11)\n",
    "axes[1].set_ylabel('Taux d\\'occupation (%)', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3, linestyle='--')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Statistiques sur le graphique\n",
    "mean_taux = df_viz['taux_occupation'].mean()\n",
    "axes[1].axhline(y=mean_taux, color='red', linestyle='--', linewidth=1.5, \n",
    "               label=f'Moyenne: {mean_taux:.1f}%', alpha=0.7)\n",
    "axes[1].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# GRAPHIQUE 2: Patterns par heure de la journ√©e\n",
    "# -------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# D√©bit horaire moyen par heure\n",
    "debit_par_heure = df_viz.groupby('heure')['debit_horaire'].agg(['mean', 'std'])\n",
    "axes[0].bar(debit_par_heure.index, debit_par_heure['mean'], \n",
    "           color='steelblue', alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "axes[0].errorbar(debit_par_heure.index, debit_par_heure['mean'], \n",
    "                yerr=debit_par_heure['std'], fmt='none', \n",
    "                ecolor='black', capsize=3, alpha=0.5)\n",
    "axes[0].set_title('D√©bit Horaire Moyen par Heure de la Journ√©e', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Heure de la journ√©e', fontsize=11)\n",
    "axes[0].set_ylabel('D√©bit horaire moyen (v√©h/h)', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_xticks(range(24))\n",
    "\n",
    "# Taux d'occupation moyen par heure\n",
    "taux_par_heure = df_viz.groupby('heure')['taux_occupation'].agg(['mean', 'std'])\n",
    "axes[1].bar(taux_par_heure.index, taux_par_heure['mean'], \n",
    "           color='darkorange', alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "axes[1].errorbar(taux_par_heure.index, taux_par_heure['mean'], \n",
    "                yerr=taux_par_heure['std'], fmt='none', \n",
    "                ecolor='black', capsize=3, alpha=0.5)\n",
    "axes[1].set_title('Taux d\\'Occupation Moyen par Heure de la Journ√©e', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Heure de la journ√©e', fontsize=11)\n",
    "axes[1].set_ylabel('Taux d\\'occupation moyen (%)', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].set_xticks(range(24))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# GRAPHIQUE 3: Comparaison Weekend vs Semaine\n",
    "# -------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# D√©bit horaire - Weekend vs Semaine\n",
    "for weekend, label, color in [(0, 'Semaine', 'steelblue'), (1, 'Weekend', 'coral')]:\n",
    "    data_subset = df_viz[df_viz['est_weekend'] == weekend]\n",
    "    debit_heure = data_subset.groupby('heure')['debit_horaire'].mean()\n",
    "    axes[0].plot(debit_heure.index, debit_heure.values, \n",
    "                marker='o', label=label, color=color, linewidth=2, markersize=6)\n",
    "\n",
    "axes[0].set_title('D√©bit Horaire - Semaine vs Weekend', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Heure de la journ√©e', fontsize=11)\n",
    "axes[0].set_ylabel('D√©bit horaire moyen (v√©h/h)', fontsize=11)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks(range(24))\n",
    "\n",
    "# Taux d'occupation - Weekend vs Semaine\n",
    "for weekend, label, color in [(0, 'Semaine', 'steelblue'), (1, 'Weekend', 'coral')]:\n",
    "    data_subset = df_viz[df_viz['est_weekend'] == weekend]\n",
    "    taux_heure = data_subset.groupby('heure')['taux_occupation'].mean()\n",
    "    axes[1].plot(taux_heure.index, taux_heure.values, \n",
    "                marker='o', label=label, color=color, linewidth=2, markersize=6)\n",
    "\n",
    "axes[1].set_title('Taux d\\'Occupation - Semaine vs Weekend', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Heure de la journ√©e', fontsize=11)\n",
    "axes[1].set_ylabel('Taux d\\'occupation moyen (%)', fontsize=11)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xticks(range(24))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# STATISTIQUES DESCRIPTIVES\n",
    "# -------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTIQUES DESCRIPTIVES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä D√âBIT HORAIRE:\")\n",
    "print(df_viz['debit_horaire'].describe())\n",
    "\n",
    "print(\"\\nüìä TAUX D'OCCUPATION:\")\n",
    "print(df_viz['taux_occupation'].describe())\n",
    "\n",
    "print(\"\\nüìä CORR√âLATION D√âBIT HORAIRE <-> TAUX D'OCCUPATION:\")\n",
    "correlation = df_viz['debit_horaire'].corr(df_viz['taux_occupation'])\n",
    "print(f\"Coefficient de corr√©lation de Pearson: {correlation:.4f}\")\n",
    "\n",
    "# Heatmap de corr√©lation\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "correlation_matrix = df_viz[['debit_horaire', 'taux_occupation']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "           center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
    "ax.set_title('Corr√©lation entre D√©bit Horaire et Taux d\\'Occupation', \n",
    "            fontsize=13, fontweight='bold', pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualisations termin√©es!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
